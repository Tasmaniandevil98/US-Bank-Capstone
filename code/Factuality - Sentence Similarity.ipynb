{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2de7af2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cands is the answer from the QA dataset\n",
    "# ref1 and ref2 are outputs from Gpt3.5 and Llama2\n",
    "\n",
    "# Question: Will capital gains affect my tax bracket?\n",
    "cands = [\"I'm not sure where you are, but in the United States capital gains are taxed at a lower rate than other types of income. On the 1040, captial gains income is separated from earned income, and income tax is calculated just on earned income. Then capital gains tax is calculated on capital gains income, and then added to income tax afterward.\"]\n",
    "ref1 = [\"As a Wealth Manager, I can assure you that capital gains can indeed impact your tax bracket. When you sell an investment for a profit, the gain is subject to capital gains tax, which can range from 0% to 20%, depending on your income tax bracket and the length of time you've held the investment. If your capital gains push your income into a higher tax bracket, you may end up paying a higher tax rate on some of your gains. However, it's worth noting that the tax implications of capital gains can be complex, and there may be strategies to minimize your tax liability, such as holding onto your investments for at least a year to qualify for long-term capital gains treatment. It's always a good idea to consult with a tax professional or financial advisor to understand the specific tax implications of your investment decisions.\"]\n",
    "ref2 = [\"Whether capital gains affect your tax bracket depends on two factors: gains type and size. Short-term gains (held less than a year) are taxed at your ordinary income rate, potentially bumping you to a higher bracket. Long-term gains (held over a year) usually have lower rates (0%, 15%, or 20%), but large gains could still push you over the threshold. Remember, it's individual, so consulting a tax professional for specific guidance is crucial.\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4431fb66",
   "metadata": {},
   "source": [
    "# BERT Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69211650",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Bad key \"text.kerning_factor\" on line 4 in\n",
      "c:\\Users\\Dibyanshu Patnaik\\.conda\\envs\\jupyter\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test_patch.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.1.3/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n"
     ]
    }
   ],
   "source": [
    "from bert_score import score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d99d159",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6d328c1a33c4aa1a28db9f641b1cd49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34e08c1f95dd49f88bc17bd26fa7ad2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 4.96 seconds, 0.20 sentences/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09dfd1f6e2234bbb80e5f7e8e5b3e9e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4807d7f3ac34865bfd7c632df33d2df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 2.51 seconds, 0.40 sentences/sec\n"
     ]
    }
   ],
   "source": [
    "P1, R1, F11 = score(cands, ref1, lang=\"en\", verbose=True)\n",
    "P2, R2, F12 = score(cands, ref2, lang=\"en\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc7101eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.8489]) tensor([0.8470])\n"
     ]
    }
   ],
   "source": [
    "print(F11, F12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4abfec2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6f7cb418",
   "metadata": {},
   "source": [
    "# Sentence Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d51ae984",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "# model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "model = SentenceTransformer(\"stsb-roberta-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa621921",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings1 = model.encode(cands, convert_to_tensor=True)\n",
    "embeddings2 = model.encode(ref1, convert_to_tensor=True)\n",
    "embeddings3 = model.encode(ref2, convert_to_tensor=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4caf5164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6036]]) tensor([[0.6389]])\n"
     ]
    }
   ],
   "source": [
    "cosine_scores1 = util.cos_sim(embeddings1, embeddings2)\n",
    "cosine_scores2 = util.cos_sim(embeddings1, embeddings3)\n",
    "\n",
    "print(cosine_scores1, cosine_scores2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0adceda1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a57fb999",
   "metadata": {},
   "source": [
    "# Jaccard Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "aae811b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.13043478260869565\n",
      "0.20238095238095238\n",
      "[[\"i'm\", 'not', 'sure', 'where', 'you', 'are,', 'but', 'in', 'the', 'united', 'states', 'capital', 'gains', 'are', 'taxed', 'at', 'a', 'lower', 'rate', 'than', 'other', 'types', 'of', 'income.', 'on', 'the', '1040,', 'captial', 'gains', 'income', 'is', 'separated', 'from', 'earned', 'income,', 'and', 'income', 'tax', 'is', 'calculated', 'just', 'on', 'earned', 'income.', 'then', 'capital', 'gains', 'tax', 'is', 'calculated', 'on', 'capital', 'gains', 'income,', 'and', 'then', 'added', 'to', 'income', 'tax', 'afterward.']]\n"
     ]
    }
   ],
   "source": [
    "def jaccard_similarity(x,y):\n",
    "  intersection_cardinality = len(set.intersection(*[set(x), set(y)]))\n",
    "  union_cardinality = len(set.union(*[set(x), set(y)]))\n",
    "  return intersection_cardinality/float(union_cardinality)\n",
    "\n",
    "print(jaccard_similarity([text.lower().split(\" \") for text in cands][0],[text.lower().split(\" \") for text in ref1][0]))\n",
    "print(jaccard_similarity([text.lower().split(\" \") for text in cands][0],[text.lower().split(\" \") for text in ref2][0]))\n",
    "# print(jaccard_similarity(cands[0].lower(),ref1[0].lower()))\n",
    "print([text.lower().split(\" \") for text in cands])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7c2bfa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74c27b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d010c9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2c0ec1b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(22.8634, grad_fn=<ExpBackward0>)\n",
      "tensor(8.9369, grad_fn=<ExpBackward0>)\n",
      "tensor(21.5526, grad_fn=<ExpBackward0>)\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(cands, return_tensors = \"pt\") #Answer in the QA Dataset\n",
    "loss = model(input_ids = inputs[\"input_ids\"], labels = inputs[\"input_ids\"]).loss\n",
    "ppl = torch.exp(loss)\n",
    "print(ppl)\n",
    "\n",
    "inputs = tokenizer(ref1, return_tensors = \"pt\")\n",
    "loss = model(input_ids = inputs[\"input_ids\"], labels = inputs[\"input_ids\"]).loss\n",
    "ppl = torch.exp(loss)\n",
    "print(ppl)\n",
    "\n",
    "inputs = tokenizer(ref2, return_tensors = \"pt\")\n",
    "loss = model(input_ids = inputs[\"input_ids\"], labels = inputs[\"input_ids\"]).loss\n",
    "ppl = torch.exp(loss)\n",
    "print(ppl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98433bd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "922ee654",
   "metadata": {},
   "source": [
    "# BLEU and ROUGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1b9a520b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "It is recommended to enable `effective_order` for sentence-level BLEU.\n",
      "It is recommended to enable `effective_order` for sentence-level BLEU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU\n",
      "0.011906943486511454\n",
      "0.02873622116900213\n",
      "ROUGE\n",
      "0.2307692263952663\n",
      "0.2772277179139301\n"
     ]
    }
   ],
   "source": [
    "from sacrebleu.metrics import BLEU\n",
    "bleu_scorer = BLEU()\n",
    "\n",
    "\n",
    "score1 = bleu_scorer.sentence_score(\n",
    "    hypothesis=cands[0],\n",
    "    references=ref1,\n",
    ")\n",
    "score2 = bleu_scorer.sentence_score(\n",
    "    hypothesis=cands[0],\n",
    "    references=ref2,\n",
    ")\n",
    "print(\"BLEU\")\n",
    "print(score1.score/100)\n",
    "print(score2.score/100)\n",
    "\n",
    "from rouge import Rouge\n",
    "rouge_scorer = Rouge()\n",
    "\n",
    "score1 = rouge_scorer.get_scores(\n",
    "    hyps=cands[0],\n",
    "    refs=ref1[0],\n",
    ")\n",
    "score2 = rouge_scorer.get_scores(\n",
    "    hyps=cands[0],\n",
    "    refs=ref2[0],\n",
    ")\n",
    "print(\"ROUGE\")\n",
    "print(score1[0][\"rouge-l\"][\"f\"])\n",
    "print(score2[0][\"rouge-l\"][\"f\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3734b792",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "af391eb4",
   "metadata": {},
   "source": [
    "# TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b2b8aa55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity using TF-IDF between embeddings 1 and 2: 0.3285035408811078\n",
      "Cosine Similarity using TF-IDF between embeddings 1 and 3: 0.31417397867087804\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Combine the texts into a single list\n",
    "all_texts = cands + ref1 + ref2\n",
    "\n",
    "# Initialize the TF-IDF vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit the vectorizer on all texts and transform each text to its TF-IDF representation\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(all_texts)\n",
    "\n",
    "# Separate the TF-IDF representations for each text\n",
    "tfidf_matrix_text1 = tfidf_matrix[:len(cands)]\n",
    "tfidf_matrix_text2 = tfidf_matrix[len(cands):len(cands)+len(ref1)]\n",
    "tfidf_matrix_text3 = tfidf_matrix[len(ref1)+len(ref1):]\n",
    "\n",
    "# Calculate cosine similarity using TF-IDF representations\n",
    "cosine_sim_tfidf1_2 = cosine_similarity(tfidf_matrix_text1, tfidf_matrix_text2)\n",
    "cosine_sim_tfidf1_3 = cosine_similarity(tfidf_matrix_text1, tfidf_matrix_text3)\n",
    "\n",
    "vectors = tfidf_vectorizer.fit_transform(cands + ref1 + ref2)\n",
    "similarity = cosine_similarity(vectors)\n",
    "\n",
    "print(similarity)\n",
    "print(\"Cosine Similarity using TF-IDF between embeddings 1 and 2:\", cosine_sim_tfidf1_2[0][0])\n",
    "print(\"Cosine Similarity using TF-IDF between embeddings 1 and 3:\", cosine_sim_tfidf1_3[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73816d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d21334b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9377119b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Prompt_Output</th>\n",
       "      <th>BERT_SCORE_LLM_OpenAI</th>\n",
       "      <th>SENTENCE_TRANSFORMERS_LLM_OpenAI</th>\n",
       "      <th>BLEU_SCORE_LLM_OpenAI</th>\n",
       "      <th>ROUGE_SCORE_LLM_OpenAI</th>\n",
       "      <th>TFIDF_LLM_OpenAI</th>\n",
       "      <th>JACCARD_SIMILARITY_LLM_OpenAI</th>\n",
       "      <th>BERT_SCORE_LLM_LLAMA</th>\n",
       "      <th>SENTENCE_TRANSFORMERS_LLM_LLAMA</th>\n",
       "      <th>BLEU_SCORE_LLM_LLAMA</th>\n",
       "      <th>...</th>\n",
       "      <th>Rank_BERT_SCORE_LLM_GEMMA</th>\n",
       "      <th>Rank_BERT_SCORE_LLM_Data4</th>\n",
       "      <th>Rank_SENTENCE_TRANSFORMERS_LLM_OpenAI</th>\n",
       "      <th>Rank_SENTENCE_TRANSFORMERS_LLM_LLAMA</th>\n",
       "      <th>Rank_SENTENCE_TRANSFORMERS_LLM_GEMMA</th>\n",
       "      <th>Rank_SENTENCE_TRANSFORMERS_LLM_Data4</th>\n",
       "      <th>Rank_JACCARD_SIMILARITY_LLM_OpenAI</th>\n",
       "      <th>Rank_JACCARD_SIMILARITY_LLM_LLAMA</th>\n",
       "      <th>Rank_JACCARD_SIMILARITY_LLM_GEMMA</th>\n",
       "      <th>Rank_JACCARD_SIMILARITY_LLM_Data4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bLAH bLAH</td>\n",
       "      <td>50.9</td>\n",
       "      <td>70</td>\n",
       "      <td>60.5</td>\n",
       "      <td>80.3</td>\n",
       "      <td>50.9</td>\n",
       "      <td>10</td>\n",
       "      <td>100</td>\n",
       "      <td>85</td>\n",
       "      <td>50.9</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Come on</td>\n",
       "      <td>100.0</td>\n",
       "      <td>65</td>\n",
       "      <td>55.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>20</td>\n",
       "      <td>15</td>\n",
       "      <td>10</td>\n",
       "      <td>100.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 49 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Prompt_Output  BERT_SCORE_LLM_OpenAI  SENTENCE_TRANSFORMERS_LLM_OpenAI  \\\n",
       "0     bLAH bLAH                   50.9                                70   \n",
       "1       Come on                  100.0                                65   \n",
       "\n",
       "   BLEU_SCORE_LLM_OpenAI  ROUGE_SCORE_LLM_OpenAI  TFIDF_LLM_OpenAI  \\\n",
       "0                   60.5                    80.3              50.9   \n",
       "1                   55.0                    95.0             100.0   \n",
       "\n",
       "   JACCARD_SIMILARITY_LLM_OpenAI  BERT_SCORE_LLM_LLAMA  \\\n",
       "0                             10                   100   \n",
       "1                             20                    15   \n",
       "\n",
       "   SENTENCE_TRANSFORMERS_LLM_LLAMA  BLEU_SCORE_LLM_LLAMA  ...  \\\n",
       "0                               85                  50.9  ...   \n",
       "1                               10                 100.0  ...   \n",
       "\n",
       "   Rank_BERT_SCORE_LLM_GEMMA  Rank_BERT_SCORE_LLM_Data4  \\\n",
       "0                          3                          2   \n",
       "1                          1                          3   \n",
       "\n",
       "   Rank_SENTENCE_TRANSFORMERS_LLM_OpenAI  \\\n",
       "0                                      3   \n",
       "1                                      2   \n",
       "\n",
       "   Rank_SENTENCE_TRANSFORMERS_LLM_LLAMA  Rank_SENTENCE_TRANSFORMERS_LLM_GEMMA  \\\n",
       "0                                     1                                     4   \n",
       "1                                     4                                     3   \n",
       "\n",
       "   Rank_SENTENCE_TRANSFORMERS_LLM_Data4  Rank_JACCARD_SIMILARITY_LLM_OpenAI  \\\n",
       "0                                     2                                   4   \n",
       "1                                     1                                   3   \n",
       "\n",
       "   Rank_JACCARD_SIMILARITY_LLM_LLAMA  Rank_JACCARD_SIMILARITY_LLM_GEMMA  \\\n",
       "0                                  1                                  3   \n",
       "1                                  4                                  2   \n",
       "\n",
       "   Rank_JACCARD_SIMILARITY_LLM_Data4  \n",
       "0                                  2  \n",
       "1                                  1  \n",
       "\n",
       "[2 rows x 49 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "consolidated_df = pd.DataFrame({\n",
    "    'Prompt_Output': [\"bLAH bLAH\", \"Come on\"],\n",
    "    'BERT_SCORE_LLM_OpenAI': [50.9, 100],\n",
    "    'SENTENCE_TRANSFORMERS_LLM_OpenAI': [70, 65],\n",
    "    'BLEU_SCORE_LLM_OpenAI': [60.5, 55],\n",
    "    'ROUGE_SCORE_LLM_OpenAI': [80.3, 95],\n",
    "    'TFIDF_LLM_OpenAI': [50.9, 100],\n",
    "    'JACCARD_SIMILARITY_LLM_OpenAI': [10, 20],\n",
    "    'BERT_SCORE_LLM_LLAMA': [100, 15],\n",
    "    'SENTENCE_TRANSFORMERS_LLM_LLAMA': [85, 10],\n",
    "    'BLEU_SCORE_LLM_LLAMA': [50.9, 100],\n",
    "    'ROUGE_SCORE_LLM_LLAMA': [10, 20],\n",
    "    'TFIDF_LLM_LLAMA': [100, 15],\n",
    "    'JACCARD_SIMILARITY_LLM_LLAMA': [85, 10],\n",
    "    'BERT_SCORE_LLM_GEMMA': [50.9, 100],\n",
    "    'SENTENCE_TRANSFORMERS_LLM_GEMMA': [10, 20],\n",
    "    'BLEU_SCORE_LLM_GEMMA': [100, 15],\n",
    "    'ROUGE_SCORE_LLM_GEMMA': [85, 10],\n",
    "    'TFIDF_LLM_GEMMA': [50.9, 100],\n",
    "    'JACCARD_SIMILARITY_LLM_GEMMA': [70, 65],\n",
    "    'BERT_SCORE_LLM_Data4': [60.5, 55],\n",
    "    'SENTENCE_TRANSFORMERS_LLM_Data4': [80.3, 95],\n",
    "    'BLEU_SCORE_LLM_Data4': [50.9, 100],\n",
    "    'ROUGE_SCORE_LLM_Data4': [70, 65],\n",
    "    'TFIDF_LLM_Data4': [60.5, 55],\n",
    "    'JACCARD_SIMILARITY_LLM_Data4': [80.3, 95],\n",
    "})\n",
    "\n",
    "# # Sort the values in each row and assign ranks for BERT_SCORE\n",
    "# consolidated_df['Rank_BERT_SCORE_LLM_OpenAI'] = consolidated_df.apply(lambda row: sorted(row.values[1:5], reverse=True).index(row['BERT_SCORE_LLM_OpenAI']) + 1, axis=1)\n",
    "# consolidated_df['Rank_BERT_SCORE_LLM_LLAMA'] = consolidated_df.apply(lambda row: sorted(row.values[1:5], reverse=True).index(row['BERT_SCORE_LLM_LLAMA']) + 1, axis=1)\n",
    "# consolidated_df['Rank_BERT_SCORE_LLM_GEMMA'] = consolidated_df.apply(lambda row: sorted(row.values[1:5], reverse=True).index(row['BERT_SCORE_LLM_GEMMA']) + 1, axis=1)\n",
    "# consolidated_df['Rank_BERT_SCORE_LLM_Data4'] = consolidated_df.apply(lambda row: sorted(row.values[1:5], reverse=True).index(row['BERT_SCORE_LLM_Data4']) + 1, axis=1)\n",
    "\n",
    "# # Sort the values in each row and assign ranks for SENTENCE_TRANSFORMERS\n",
    "# consolidated_df['Rank_SENTENCE_TRANSFORMERS_LLM_OpenAI'] = consolidated_df.apply(lambda row: sorted(row.values[5:], reverse=True).index(row['SENTENCE_TRANSFORMERS_LLM_OpenAI']) + 1, axis=1)\n",
    "# consolidated_df['Rank_SENTENCE_TRANSFORMERS_LLM_LLAMA'] = consolidated_df.apply(lambda row: sorted(row.values[5:], reverse=True).index(row['SENTENCE_TRANSFORMERS_LLM_LLAMA']) + 1, axis=1)\n",
    "# consolidated_df['Rank_SENTENCE_TRANSFORMERS_LLM_GEMMA'] = consolidated_df.apply(lambda row: sorted(row.values[5:], reverse=True).index(row['SENTENCE_TRANSFORMERS_LLM_GEMMA']) + 1, axis=1)\n",
    "# consolidated_df['Rank_SENTENCE_TRANSFORMERS_LLM_Data4'] = consolidated_df.apply(lambda row: sorted(row.values[5:], reverse=True).index(row['SENTENCE_TRANSFORMERS_LLM_Data4']) + 1, axis=1)\n",
    "\n",
    "# consolidated_df\n",
    "\n",
    "# Rank the values based on the prefix of column names\n",
    "prefixes = set(col.split('_')[0] for col in consolidated_df.columns[1:])\n",
    "for prefix in prefixes:\n",
    "    prefix_columns = [col for col in consolidated_df.columns if col.startswith(prefix)]\n",
    "    ranks = consolidated_df[prefix_columns].rank(axis=1, method='min', ascending=False)\n",
    "    rank_columns = [f'Rank_{col}' for col in prefix_columns]\n",
    "    consolidated_df[rank_columns] = ranks.astype(int)\n",
    "\n",
    "consolidated_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
